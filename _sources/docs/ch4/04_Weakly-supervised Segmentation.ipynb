{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Overview\n",
        "\n",
        "## Introduction\n",
        "\n",
        ":::{figure-md} markdown-fig\n",
        "<img src=\"pic/weakly1.png\" alt=\"weakly1\" class=\"bg-primary mb-1\" width=\"800px\">\n",
        "\n",
        "Weakly supervised learning. (source: Hakan, CVPR18 Tutorial)\n",
        ":::\n",
        "\n",
        "- Weakly-supervised learning은 full supervision이 아닌 완벽하지 않은 저수준(lower degree, noisy, limited, or imprecise sources)의 supervision 이용하여 학습방법이다.\n",
        "- Segmentation task의 경우 supervised leaning 기반 학습 setting에서는 학습데이터로 주어진 모든 영상 정보에 대해 pixel level의 class label이 필요하며, 이러한 고수준의 labeled data가 필요한 task에서 data를 확보하는 과정에서 다음과 같은 문제가 발생할 수 있다.\n",
        "    \n",
        "    (1) 충분한 양의 labeled data를 확보하기 어렵다.\n",
        "\n",
        "    (2) 특정 task의 경우 label data 생성에 전문가가 필요하고, 이는 많은 비용과 노력을 필요로 한다. (e.g.의료 영상)\n",
        "\n",
        "    (3) 충분한 Data를 수집하기 위한 시간이 많이 든다.\n",
        "    \n",
        "- Weakly supervised learning 기반 방법에서는 영상 전체의 classification 결과 (class label), 또는 object detection 결과 (bounding box, class label)와 같은 full supervision 대비 저수준의 labeled data를 이용하여 semantic/instance segmentation task와 같은 고수준의 task를 수행하는 network를 학습한다. 이를 통해 data 수집에 필요한 비용과 시간을 절감할 수 있다.\n",
        "- Pixel/instance level의 label이 확보되지 않는 weakly supervised setting에서는 Class attention map (CAM)을 주로 활용한다.\n",
        "\n",
        ":::{figure-md} markdown-fig\n",
        "<img src=\"pic/weakly2.png\" alt=\"weakly2\" class=\"bg-primary mb-1\" width=\"800px\">\n",
        "\n",
        "Class Attention Map (source: Learning Deep Features for Discriminative Localization, CVPR 2016)\n",
        ":::\n",
        "\n",
        "- CAM은 Network가 image를 특정 class로 prediction하게 하는 feature 내의 위치 정보를 표현한다.\n",
        "- 마지막 conv. layer의 output feature map $f^{\\text{CAM}}$ 특정 class로 분류될 activation value를 구하기 위한 weight와 weighted sum을 수행하여 CAM을 도출한다.\n",
        "\n",
        "- 이러한 CAM 정보는 class label만이 주어진 환경에서 이미지 내에 객체가 어디에 속한지 알려주지만 Classification을 위한 network에서 특정 object에서 나타나는 특정한 pattern에 대해 score가 학습이 되므로, 다양한 training sample에서 공통적으로 나타나는 scene들에 대해서 score가 낮게 학습이 된다. \n",
        "\n",
        "- 즉 배경과 잘 구분되는 object와 같이 discriminate한 부분에 집중하여 학습이 되므로, sparse하고 blurrly한 정보를 포함하고 있으며, score가 object의 외각영역에 집중되는 경향이 있다. \n",
        "\n",
        ":::{figure-md} markdown-fig\n",
        "<img src=\"pic/weakly3.png\" alt=\"weakly3\" class=\"bg-primary mb-1\" width=\"800px\">\n",
        "\n",
        "Examples of Class Attention Map (source: Weakly Supervised Object Detection, ECCV Tutorial 2020)\n",
        ":::\n",
        "        \n",
        "- 이를 해결하기 위해 다양한 기법의 weakly supervised learning 기법이 제안되었고, 이를 분류해보면 다음과 같다.\n",
        "    \n",
        "    (1) Seeded Region Growing\n",
        "    \n",
        "    (2) Adversarial Erasing\n",
        "    \n",
        "    (3) Learning Pixel Affinity\n",
        "    \n",
        "    (4) Sailency or Attention Mechanism\n",
        "    \n",
        "+++ {\"meta\": \"data\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [
          "remove-input"
        ]
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# https://medium.com/analytics-vidhya/creating-a-dual-axis-pareto-chart-in-altair-e3673107dd14\n",
        "# https://altair-viz.github.io/user_guide/interactions.html\n",
        "# https://www.datacamp.com/tutorial/altair-in-python\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import altair as alt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def GetGraphElement(chart_title, data, x_scale, y_scale, perf_measure, line_color = \"#000000\", point_color = \"#000000\", text_color = \"#000000\", text_y_pos = -10, textperf_y_pos=-20):\n",
        "    base = alt.Chart(data).encode(\n",
        "    x = alt.X(\"idx\", scale=alt.Scale(domain=x_scale),axis=None),\n",
        "    ).properties (\n",
        "    width = 800,\n",
        "    title = [chart_title]\n",
        "    )\n",
        "\n",
        "    line = base.mark_line(strokeWidth= 1.5, color = line_color).encode(\n",
        "        y=alt.Y(perf_measure, scale=alt.Scale(domain=y_scale),axis=alt.Axis(grid=True)),\n",
        "        color=alt.Color('type'),\n",
        "    )\n",
        "\n",
        "    points = base.mark_circle(strokeWidth= 3, color = point_color).encode(\n",
        "            y=alt.Y(perf_measure, scale=alt.Scale(domain=y_scale), axis=None),\n",
        "            tooltip = [alt.Tooltip('year'),\n",
        "            alt.Tooltip('nickname'),\n",
        "            alt.Tooltip(perf_measure),\n",
        "            alt.Tooltip('note'),],\n",
        "    )\n",
        "\n",
        "    point_nick = points.mark_text(align='center', baseline='middle', dy = text_y_pos,).encode(\n",
        "        y= alt.Y(perf_measure, scale=alt.Scale(domain=y_scale), axis=None),\n",
        "        text=alt.Text(perf_measure),\n",
        "        color= alt.value(text_color)\n",
        "    )\n",
        "    point_perf = points.mark_text(align='center', baseline='middle', dy = textperf_y_pos).encode(\n",
        "        y= alt.Y(perf_measure, scale=alt.Scale(domain=y_scale), axis=None),\n",
        "        text=alt.Text('nickname'),\n",
        "        color= alt.value(text_color)\n",
        "    )   \n",
        "\n",
        "    return base, line, points, point_nick, point_perf\n",
        "\n",
        "def description_test(pos_x, pos_y, text, color):\n",
        "    return alt.Chart({'values':[{}]}).mark_text(align = \"left\", baseline =\"top\").encode(\n",
        "        x = alt.value(pos_x),\n",
        "        y = alt.value(pos_y),\n",
        "        text = alt.value([text]),\n",
        "        color= alt.value(color)\n",
        "    )\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "tags": [
          "remove-input"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-f7ea8fef1843406baa4b37d688fa0fe3\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-f7ea8fef1843406baa4b37d688fa0fe3\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-f7ea8fef1843406baa4b37d688fa0fe3\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"line\", \"color\": \"#fde725\", \"strokeWidth\": 1.5}, \"encoding\": {\"color\": {\"field\": \"type\", \"type\": \"nominal\"}, \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 7.0]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": {\"grid\": true}, \"field\": \"miou\", \"scale\": {\"domain\": [55.0, 90.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mIoU (PASCAL VOC 2012 testset)\"], \"width\": 800}, {\"mark\": {\"type\": \"circle\", \"color\": \"#000000\", \"strokeWidth\": 3}, \"encoding\": {\"tooltip\": [{\"field\": \"year\", \"type\": \"quantitative\"}, {\"field\": \"nickname\", \"type\": \"nominal\"}, {\"field\": \"miou\", \"type\": \"quantitative\"}, {\"field\": \"note\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 7.0]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": null, \"field\": \"miou\", \"scale\": {\"domain\": [55.0, 90.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mIoU (PASCAL VOC 2012 testset)\"], \"width\": 800}, {\"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"dy\": -20}, \"encoding\": {\"color\": {\"value\": \"#000000\"}, \"text\": {\"field\": \"miou\", \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"year\", \"type\": \"quantitative\"}, {\"field\": \"nickname\", \"type\": \"nominal\"}, {\"field\": \"miou\", \"type\": \"quantitative\"}, {\"field\": \"note\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 7.0]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": null, \"field\": \"miou\", \"scale\": {\"domain\": [55.0, 90.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mIoU (PASCAL VOC 2012 testset)\"], \"width\": 800}, {\"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"dy\": -30}, \"encoding\": {\"color\": {\"value\": \"#000000\"}, \"text\": {\"field\": \"nickname\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"year\", \"type\": \"quantitative\"}, {\"field\": \"nickname\", \"type\": \"nominal\"}, {\"field\": \"miou\", \"type\": \"quantitative\"}, {\"field\": \"note\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 7.0]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": null, \"field\": \"miou\", \"scale\": {\"domain\": [55.0, 90.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mIoU (PASCAL VOC 2012 testset)\"], \"width\": 800}, {\"data\": {\"name\": \"data-a7a59e6ea057dc8e66898266a805f932\"}, \"mark\": {\"type\": \"line\", \"color\": \"#cb4154\", \"strokeWidth\": 1.5}, \"encoding\": {\"color\": {\"field\": \"type\", \"type\": \"nominal\"}, \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 7.0]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": {\"grid\": true}, \"field\": \"miou\", \"scale\": {\"domain\": [55.0, 90.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mIoU (PASCAL VOC 2012 testset)\"], \"width\": 800}, {\"data\": {\"name\": \"data-a7a59e6ea057dc8e66898266a805f932\"}, \"mark\": {\"type\": \"circle\", \"color\": \"#000000\", \"strokeWidth\": 3}, \"encoding\": {\"tooltip\": [{\"field\": \"year\", \"type\": \"quantitative\"}, {\"field\": \"nickname\", \"type\": \"nominal\"}, {\"field\": \"miou\", \"type\": \"quantitative\"}, {\"field\": \"note\", \"type\": \"quantitative\"}], \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 7.0]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": null, \"field\": \"miou\", \"scale\": {\"domain\": [55.0, 90.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mIoU (PASCAL VOC 2012 testset)\"], \"width\": 800}, {\"data\": {\"name\": \"data-a7a59e6ea057dc8e66898266a805f932\"}, \"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"dy\": -10}, \"encoding\": {\"color\": {\"value\": \"#000000\"}, \"text\": {\"field\": \"miou\", \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"year\", \"type\": \"quantitative\"}, {\"field\": \"nickname\", \"type\": \"nominal\"}, {\"field\": \"miou\", \"type\": \"quantitative\"}, {\"field\": \"note\", \"type\": \"quantitative\"}], \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 7.0]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": null, \"field\": \"miou\", \"scale\": {\"domain\": [55.0, 90.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mIoU (PASCAL VOC 2012 testset)\"], \"width\": 800}, {\"data\": {\"name\": \"data-a7a59e6ea057dc8e66898266a805f932\"}, \"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"dy\": 20}, \"encoding\": {\"color\": {\"value\": \"#000000\"}, \"text\": {\"field\": \"nickname\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"year\", \"type\": \"quantitative\"}, {\"field\": \"nickname\", \"type\": \"nominal\"}, {\"field\": \"miou\", \"type\": \"quantitative\"}, {\"field\": \"note\", \"type\": \"quantitative\"}], \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 7.0]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": null, \"field\": \"miou\", \"scale\": {\"domain\": [55.0, 90.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mIoU (PASCAL VOC 2012 testset)\"], \"width\": 800}, {\"data\": {\"name\": \"data-f995d10f95b820be74e1f67a725082f1\"}, \"mark\": {\"type\": \"line\", \"color\": \"#3b518b\", \"strokeWidth\": 1.5}, \"encoding\": {\"color\": {\"field\": \"type\", \"type\": \"nominal\"}, \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 7.0]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": {\"grid\": true}, \"field\": \"miou\", \"scale\": {\"domain\": [55.0, 90.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mIoU (PASCAL VOC 2012 testset)\"], \"width\": 800}, {\"data\": {\"name\": \"data-f995d10f95b820be74e1f67a725082f1\"}, \"mark\": {\"type\": \"circle\", \"color\": \"#000000\", \"strokeWidth\": 3}, \"encoding\": {\"tooltip\": [{\"field\": \"year\", \"type\": \"quantitative\"}, {\"field\": \"nickname\", \"type\": \"nominal\"}, {\"field\": \"miou\", \"type\": \"quantitative\"}, {\"field\": \"note\", \"type\": \"quantitative\"}], \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 7.0]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": null, \"field\": \"miou\", \"scale\": {\"domain\": [55.0, 90.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mIoU (PASCAL VOC 2012 testset)\"], \"width\": 800}, {\"data\": {\"name\": \"data-f995d10f95b820be74e1f67a725082f1\"}, \"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"dy\": 20}, \"encoding\": {\"color\": {\"value\": \"#000000\"}, \"text\": {\"field\": \"miou\", \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"year\", \"type\": \"quantitative\"}, {\"field\": \"nickname\", \"type\": \"nominal\"}, {\"field\": \"miou\", \"type\": \"quantitative\"}, {\"field\": \"note\", \"type\": \"quantitative\"}], \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 7.0]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": null, \"field\": \"miou\", \"scale\": {\"domain\": [55.0, 90.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mIoU (PASCAL VOC 2012 testset)\"], \"width\": 800}, {\"data\": {\"name\": \"data-f995d10f95b820be74e1f67a725082f1\"}, \"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"dy\": 30}, \"encoding\": {\"color\": {\"value\": \"#000000\"}, \"text\": {\"field\": \"nickname\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"year\", \"type\": \"quantitative\"}, {\"field\": \"nickname\", \"type\": \"nominal\"}, {\"field\": \"miou\", \"type\": \"quantitative\"}, {\"field\": \"note\", \"type\": \"quantitative\"}], \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 7.0]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": null, \"field\": \"miou\", \"scale\": {\"domain\": [55.0, 90.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mIoU (PASCAL VOC 2012 testset)\"], \"width\": 800}], \"data\": {\"name\": \"data-c508f9b8af08f1a2c2cd8b1121f4df96\"}, \"resolve\": {\"scale\": {\"y\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-c508f9b8af08f1a2c2cd8b1121f4df96\": [{\"idx\": 1, \"type\": \"Full Supervision\", \"year\": 2015, \"nickname\": \"FCN\", \"miou\": 62.2, \"note\": \"VGG16\"}, {\"idx\": 2, \"type\": \"Full Supervision\", \"year\": 2017, \"nickname\": \"PSPNet\", \"miou\": 85.4, \"note\": \"COCO-pretrained ResNet-101\"}, {\"idx\": 3, \"type\": \"Full Supervision\", \"year\": 2020, \"nickname\": \"DeepLab_v3\", \"miou\": 85.7, \"note\": \"COCO-pretrained ResNet-101\"}, {\"idx\": 4, \"type\": \"Full Supervision\", \"year\": 2020, \"nickname\": \"DeepLab_v3+\", \"miou\": 89.0, \"note\": \"JFT pretrained Xception-65\"}], \"data-a7a59e6ea057dc8e66898266a805f932\": [{\"idx\": 3, \"type\": \"WS-learning pixel affinity\", \"year\": 2017, \"nickname\": \"RAKW\", \"miou\": 62.2, \"note\": null}, {\"idx\": 4, \"type\": \"WS-learning pixel affinity\", \"year\": 2018, \"nickname\": \"AffinityNet\", \"miou\": 63.7, \"note\": null}, {\"idx\": 5, \"type\": \"WS-learning pixel affinity\", \"year\": 2019, \"nickname\": \"IRNet\", \"miou\": 64.8, \"note\": null}, {\"idx\": 6, \"type\": \"WS-learning pixel affinity\", \"year\": 2020, \"nickname\": \"CIAN\", \"miou\": 65.3, \"note\": null}], \"data-f995d10f95b820be74e1f67a725082f1\": [{\"idx\": 3, \"type\": \"WS-adversarial erasing\", \"year\": 2017, \"nickname\": \"AE-PSL\", \"miou\": 55.7, \"note\": null}, {\"idx\": 4, \"type\": \"WS-adversarial erasing\", \"year\": 2018, \"nickname\": \"SeeNet\", \"miou\": 62.8, \"note\": null}, {\"idx\": 6, \"type\": \"WS-adversarial erasing\", \"year\": 2020, \"nickname\": \"EADER\", \"miou\": 63.8, \"note\": null}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.LayerChart(...)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"weakly_semantic_trend.csv\", sep=\",\")\n",
        "\n",
        "full_data = data.loc[data['type'] ==\"Full Supervision\"]\n",
        "affinity_data = data.loc[data['type'] ==\"WS-learning pixel affinity\"]\n",
        "adv_data = data.loc[data['type'] ==\"WS-adversarial erasing\"]\n",
        "\n",
        "perf_measure = 'miou'\n",
        "\n",
        "x_scale = [0,data['idx'].max()+1]\n",
        "y_scale = [((data[perf_measure].min()//5))*5,((data[perf_measure].max()//5)+1)*5]\n",
        "\n",
        "chart_title = \"Trend on mIoU (PASCAL VOC 2012 testset)\"\n",
        "\n",
        "base, line, points, point_nick, point_perf = GetGraphElement(chart_title, full_data, x_scale, y_scale, perf_measure, \n",
        "                                                            line_color = \"#fde725\", point_color = \"#000000\", text_color = \"#000000\", \n",
        "                                                            text_y_pos = -20, textperf_y_pos=-30)\n",
        "base2, line2, points2, point_nick2, point_perf2 = GetGraphElement(chart_title, affinity_data, x_scale, y_scale, perf_measure, \n",
        "                                                                    line_color = \"#cb4154\", point_color = \"#000000\", text_color = \"#000000\", \n",
        "                                                                    text_y_pos = -10, textperf_y_pos=20)\n",
        "base3, line3, points3, point_nick3, point_perf3 = GetGraphElement(chart_title, adv_data, x_scale, y_scale, perf_measure, \n",
        "                                                                    line_color = \"#3b518b\", point_color = \"#000000\", text_color = \"#000000\", \n",
        "                                                                    text_y_pos = 20, textperf_y_pos=30)\n",
        "(\n",
        "    line+points+point_nick+point_perf+\n",
        "    line2+points2+point_nick2+point_perf2+ \n",
        "    line3+points3+point_nick3+point_perf3\n",
        ").resolve_scale(y = 'independent')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "+++\n",
        "\n",
        " - 본 chapter에서는 위 분류에 속한 다양한 weakly-supervised learning 기반 segmentation 연구결과를 리뷰한다.\n",
        "\n",
        "*Latest update: Nov. 10, 2022*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Panoptic_Segmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
