# [Draft] BCNet - CVPR 2021

---

- **Title:** Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers

- **Review By:** Hwigeon Oh

- **Edited by:** 

---

## Reference

- paper : [https://arxiv.org/abs/2103.12340](https://arxiv.org/abs/2103.12340)
- code : [https://github.com/lkeab/BCNet](https://github.com/lkeab/BCNet)

# 1. Introduction

- 많은 segmentation error는 overlapping object에서 발생한다. 특히 같은 class의 object가 overlap 된 경우에 더 심하다.

- 본 논문에서는 occluder / occludee 를 나누는 layer를 추가하는 것으로 해당 문제를 해결하고자 한다.

- 해당 구조를 Bilayer Occluder-Occludee structure 라고 하고 있으며, RoI Head 부분에 위치시켰다.

:::{figure-md} markdown-fig
<img src="pic/bcnet/20211027185503.png" alt="figure1" class="bg-primary mb-1" width="600px">

BCNet : Mask head architecture
:::

:::{figure-md} markdown-fig
<img src="pic/bcnet/20211027185904.png" alt="figure2" class="bg-primary mb-1" width="600px">

Architecture of our BCNet with bilayer occluder-occludee relational modeling
:::

# 2. Occlusion-Aware Instance Segmentation

## Bilayer Occluder-Occludee Modeling

- Represent the graph convolution operation as, 

$$Z = \sigma(AXW_{g}) + X$$

- $X \in \mathbb{R}^{N \times K}$ is the input feature

- $N=H \times W$ is the number of pixel grids within the ROI region

- $A \in \mathbb{R}^{N \times N}$ is the adjacency matrix

- $W_{g} \in \mathbb{R}^{K \times K^{'}}$ is the learnable weight matrix

- Adjacency matrix $A$는 graph node 사이의 similarity로 정의 된다.
	
- $A_{ij} = \text{softmax}(F(\textbf{x}_{i}, \textbf{x}_{j}))$
	
- $F(\textbf{x}_{i}, \textbf{x}_{j}) = \theta(\textbf{x}_{i})^{T} \phi(\textbf{x}_{j})$
	

- 아무리 봐도 attention과 다를바가 없는 것 같아서 코드를 확인해보니 attention하고 동일하다.
	
```python
# https://github.com/lkeab/BCNet/blob/main/detectron2/modeling/roi_heads/mask_head.py#L411

# x: B,C,H,W
# x_query: B,C,HW
x_query_bound = self.query_transform_bound(x).view(B, C, -1)

# x_query: B,HW,C
x_query_bound = torch.transpose(x_query_bound, 1, 2)

# x_key: B,C,HW
x_key_bound = self.key_transform_bound(x).view(B, C, -1)

# x_value: B,C,HW
x_value_bound = self.value_transform_bound(x).view(B, C, -1)

# x_value: B,HW,C
x_value_bound = torch.transpose(x_value_bound, 1, 2)

# W = Q^T K: B,HW,HW
x_w_bound = torch.matmul(x_query_bound, x_key_bound) * self.scale
x_w_bound = F.softmax(x_w_bound, dim=-1)

# x_relation = WV: B,HW,C
x_relation_bound = torch.matmul(x_w_bound, x_value_bound)

# x_relation = B,C,HW
x_relation_bound = torch.transpose(x_relation_bound, 1, 2)

# x_relation = B,C,H,W
x_relation_bound = x_relation_bound.view(B,C,H,W)
x_relation_bound = self.output_transform_bound(x_relation_bound)
x_relation_bound = self.blocker_bound(x_relation_bound)

x = x + x_relation_bound
```

- $Z^{0} = \sigma(A^{0} X_{\text{roi}} W_{g}^{0}) + X_{\text{roi}}$

- $X_{f} = Z^{0}W_{f}^{0} + X_{\text{roi}}$

- $Z^{1} = \sigma(A^{1}X_{f}W_{g}^{1}) + X_{f}$


- Occluder feature ($Z^{0}$)를 추출한 뒤에, 그 feature를 Occludee feature($Z^{1}$)를 만들 때 사용한다.

<br>

## End-to-end Patameter Learning

- Backbone ~ RoI Head는 FCOS를 사용했다.

- Mask Head 부분은 mask-rcnn과 거의 비슷하다.

	- 각 class에 대해 BCE로 학습한다.

	- Boundary에 대한 loss가 추가 된다.

	- Occluder region은 offline으로 미리 구해둔다.


	```python
	# https://github.com/lkeab/BCNet/blob/main/detectron2/data/datasets/process_dataset.py#L254:L296
	
	for index1, a_box in enumerate(box_list):
		union_mask_whole = np.zeros((int(img_dict["height"]), int(img_dict["width"])), dtype=int)
		for index2, b_box in enumerate(box_list):
			if index1 != index2:
				iou = bb_intersection_over_union(a_box, b_box)
				if iou > 0.05:
					union_mask = np.multiply(box_mask_list[index1], bitmask_list[index2])
					union_mask_whole += union_mask

		print("===========================================")
		print('bit mask area:', bitmask_list[index1].sum())
		union_mask_whole[union_mask_whole > 1.0] = 1.0
		print('cropped union mask area:', union_mask_whole.sum())
		intersect_mask = union_mask_whole * bitmask_list[index1]
		print('intersect mask area:', intersect_mask.sum()) 
		print('intersect rate:', intersect_mask.sum()/float(bitmask_list[index1].sum()))
		print("===========================================")

		if intersect_mask.sum() >= 1.0:
			intersect_num += 1

		if float(bitmask_list[index1].sum()) > 1.0:
			intersect_rate += intersect_mask.sum()/float(bitmask_list[index1].sum())

		union_mask_non_zero_num = np.count_nonzero(union_mask_whole.astype(int))
		record["annotations"][index1]['bg_object_segmentation'] = []
		if union_mask_non_zero_num > 20:
			sum_co_box += 1
			contours = measure.find_contours(union_mask_whole.astype(int), 0)
			for contour in contours:
				if contour.shape[0] > 500: 
					contour = np.flip(contour, axis=1)[::10,:]
				elif contour.shape[0] > 200: 
					contour = np.flip(contour, axis=1)[::5,:]
				elif contour.shape[0] > 100: 
					contour = np.flip(contour, axis=1)[::3,:]
				elif contour.shape[0] > 50: 
					contour = np.flip(contour, axis=1)[::2,:]
				else:
					contour = np.flip(contour, axis=1)

				segmentation = contour.ravel().tolist()
				record["annotations"][index1]['bg_object_segmentation'].append(segmentation)
	
	```
	
- target object를 occludee로 설정한다.

- target object box안에 있는 다른 object pixel을 전부 union 하는 것으로 occluder region를 구한다.

- 따라서, 실제로는 occlusion이 일어나지 않아도 occluder가 될 수 있다.

- 참고: [link](https://github.com/lkeab/BCNet/issues/11)

```{image} pic/bcnet/20211027194937.png
:alt: 20211027194937.png
:class: bg-primary mb-1
:align: center
```

- 이후 occluder region은 ground truth로 이용된다.

- bondary ground truth는 online으로 구한다.

	- https://github.com/lkeab/BCNet/blob/main/detectron2/modeling/roi_heads/mask_head.py#L83:L89

	- https://github.com/lkeab/BCNet/blob/main/detectron2/layers/boundary.py#L38:L59

	- https://github.com/lkeab/BCNet/blob/main/detectron2/layers/boundary.py#L12:L26

	- skimage의 `binary_dilation` 함수를 통해서 counter를 약간 두껍게 만든다.



# 4. Experiments

```{image} pic/bcnet/20211027201227.png
:alt: 20211027201227.png
:class: bg-primary mb-1
:align: center
```