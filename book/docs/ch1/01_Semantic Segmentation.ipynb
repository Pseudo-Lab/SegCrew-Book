{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ7qhNBrevco"
      },
      "source": [
        "# Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Overview\n",
        "\n",
        "## Introduction\n",
        "\n",
        "- Semantic segmentation이란 영상을 의미적/인지적 단위로 구분하여 분할하는 기술을 의미한다. 이는 각 pixel이 어떤 semantic을 가지는지 분류하는 pixel level classification 문제로 해석할 수 있으며, 모든 pixel에 대한 prediction을 수행하기 때문에 dense prediction으로 불리우기도 한다. \n",
        "\n",
        ":::{figure-md} markdown-fig\n",
        "<img src=\"pic/seg1.png\" alt=\"seg1\" class=\"bg-primary mb-1\" width=\"600px\">\n",
        "\n",
        "Semantic Segmentation \\\n",
        "(source: https://tariq-hasan.github.io/concepts/computer-vision-semantic-segmentation/)\n",
        ":::\n",
        "\n",
        "- VGG, ResNet과 같은 classification task를 위한 network는 입력영상에 대해 convolution layer와 pooling layer를 단계적으로 거치며, feature를 추출한다. \n",
        "\n",
        "- Pooling layer는 입력되는 feature map을 축소하는 과정으로 큰 receptive field를 갖도록 하여 점진적으로 전역적인 특징을 (global feature)를 추출할 수 있도록 고안되어 있지만, feature의 resolution 역시 줄어들기 때문에 segmentation task에는 필연적으로 정보의 손실을 야기한다. \n",
        "\n",
        "- 또한 classification network의 마지막 layer에서는 global average pooling(GAP) 또는 fully connected layer(FCN) 등을 이용하여 목표 class 수 만큼 node를 축소한 후 one-hot encoding을 통해 최종 classification 결과를 도출하는 과정을 거치게 되는데, 이 과정에서 위치별 activation이 소실되기 때문에 dense prediction을 수행할 수 없게된다. \n",
        "\n",
        "+++\n",
        "\n",
        "- 2014년에 classification task를 위해 학습된 VGG network의 마지막 layer를 1x1 convolution layer으로 교체하여 위치 정보가 포함된 activation을 추출한 후 이를 upsample하는 방법인 FCN(Title: Fully Convolutional Network for Semantic Segmentation)가 제안되면서 이를 기반으로 다양한 semantic segmentation architecture가 제안되었다. \n",
        "\n",
        ":::{figure-md} markdown-fig\n",
        "<img src=\"pic/seg2.png\" alt=\"seg2\" class=\"bg-primary mb-1\" width=\"600px\">\n",
        "\n",
        "Comparison of a CNN for classification (top) and a FCN which creates a heat map (bottom) (source: arXiv:1511.00513)\n",
        ":::\n",
        "\n",
        "- Neural network, 특히 convolutional neural network(CNN)을 이용한 sementic segmentation을 구성하기 위해 고려해야할 사항은 다음과 같다. \n",
        "  \n",
        "    (1) 넓은 receptive field를 유지하면서 고해상도 feature를 추출하기 위한 architecture 설계 \n",
        "  \n",
        "    (2) 원본과 동일한 segmentation 결과를 얻기 위한 upsample 방법의 선택\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Measure\n",
        "\n",
        "- Semantic Segmentation의 성능 평가 지표로 가장 일반적으로 사용되는 지표는 IoU(Intersection-over-Union) 이다. \n",
        "\n",
        ":::{figure-md} iou-fig\n",
        "<img src=\"pic/IoU.png\" alt=\"iou\" class=\"bg-primary mb-1\" width=\"300px\">\n",
        "\n",
        "Intersection-over-Union (source: Wikipedia)\n",
        ":::\n",
        "\n",
        "- Jaccard 인덱스라고도 하는 IoU는  predict한 segmentation과 ground truth 간의 중첩 영역을 합집합 영역으로 나눈 것이다. 범위는 0–1(0–100%)이며 0은 겹치지 않음을 나타내고 완벽하게 segmentation된 경우 1을 나타낸다\n",
        "\n",
        "$$\\text{IoU}=\\frac{\\text{TP}}{\\text{TP}+\\text{FP}+\\text{FN}}$$\n",
        "\n",
        "- Image segmentation task의 경우 각 class 별로 IoU를 계산 후 평균을 취한 mIoU(mean IoU)를 평가 지표로 활용한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "tags": [
          "remove-input"
        ]
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# https://medium.com/analytics-vidhya/creating-a-dual-axis-pareto-chart-in-altair-e3673107dd14\n",
        "# https://altair-viz.github.io/user_guide/interactions.html\n",
        "# https://www.datacamp.com/tutorial/altair-in-python\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import altair as alt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def GetGraphElement(chart_title, data, x_scale, y_scale, perf_measure, line_color = \"#000000\", point_color = \"#000000\", text_color = \"#000000\", text_y_pos = -10, textperf_y_pos=-20):\n",
        "    base = alt.Chart(data).encode(\n",
        "    x = alt.X(\"idx\", scale=alt.Scale(domain=x_scale),axis=None),\n",
        "    ).properties (\n",
        "    width = 600,\n",
        "    title = [chart_title]\n",
        "    )\n",
        "\n",
        "    line = base.mark_line(strokeWidth= 1.5, color = line_color).encode(\n",
        "        y=alt.Y(perf_measure, scale=alt.Scale(domain=y_scale),axis=alt.Axis(grid=True)),\n",
        "        color=alt.Color('type'),\n",
        "    )\n",
        "\n",
        "    points = base.mark_circle(strokeWidth= 3, color = point_color).encode(\n",
        "            y=alt.Y(perf_measure, scale=alt.Scale(domain=y_scale), axis=None),\n",
        "            tooltip = [alt.Tooltip('year'),\n",
        "            alt.Tooltip('nickname'),\n",
        "            alt.Tooltip(perf_measure),\n",
        "            alt.Tooltip('note'),],\n",
        "    )\n",
        "\n",
        "    point_nick = points.mark_text(align='center', baseline='middle', dy = text_y_pos,).encode(\n",
        "        y= alt.Y(perf_measure, scale=alt.Scale(domain=y_scale), axis=None),\n",
        "        text=alt.Text(perf_measure),\n",
        "        color= alt.value(text_color)\n",
        "    )\n",
        "    point_perf = points.mark_text(align='center', baseline='middle', dy = textperf_y_pos).encode(\n",
        "        y= alt.Y(perf_measure, scale=alt.Scale(domain=y_scale), axis=None),\n",
        "        text=alt.Text('nickname'),\n",
        "        color= alt.value(text_color)\n",
        "    )   \n",
        "\n",
        "    return base, line, points, point_nick, point_perf\n",
        "\n",
        "def description_test(pos_x, pos_y, text, color):\n",
        "    return alt.Chart({'values':[{}]}).mark_text(align = \"left\", baseline =\"top\").encode(\n",
        "        x = alt.value(pos_x),\n",
        "        y = alt.value(pos_y),\n",
        "        text = alt.value([text]),\n",
        "        color= alt.value(color)\n",
        "    )\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "tags": [
          "remove-input"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-f9557e1e5b9d47918bbeb7abd3b6c7b4\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-f9557e1e5b9d47918bbeb7abd3b6c7b4\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-f9557e1e5b9d47918bbeb7abd3b6c7b4\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"line\", \"color\": \"#fde725\", \"strokeWidth\": 1.5}, \"encoding\": {\"color\": {\"field\": \"type\", \"type\": \"nominal\"}, \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 8]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": {\"grid\": true}, \"field\": \"miou\", \"scale\": {\"domain\": [60.0, 90.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mean IoU (PASCAL VOC 2012 Test-dev)\"], \"width\": 800}, {\"mark\": {\"type\": \"circle\", \"color\": \"#000000\", \"strokeWidth\": 3}, \"encoding\": {\"tooltip\": [{\"field\": \"year\", \"type\": \"quantitative\"}, {\"field\": \"nickname\", \"type\": \"nominal\"}, {\"field\": \"miou\", \"type\": \"quantitative\"}, {\"field\": \"note\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 8]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": null, \"field\": \"miou\", \"scale\": {\"domain\": [60.0, 90.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mean IoU (PASCAL VOC 2012 Test-dev)\"], \"width\": 800}, {\"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"dy\": -10}, \"encoding\": {\"color\": {\"value\": \"#000000\"}, \"text\": {\"field\": \"miou\", \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"year\", \"type\": \"quantitative\"}, {\"field\": \"nickname\", \"type\": \"nominal\"}, {\"field\": \"miou\", \"type\": \"quantitative\"}, {\"field\": \"note\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 8]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": null, \"field\": \"miou\", \"scale\": {\"domain\": [60.0, 90.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mean IoU (PASCAL VOC 2012 Test-dev)\"], \"width\": 800}, {\"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"dy\": 20}, \"encoding\": {\"color\": {\"value\": \"#000000\"}, \"text\": {\"field\": \"nickname\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"year\", \"type\": \"quantitative\"}, {\"field\": \"nickname\", \"type\": \"nominal\"}, {\"field\": \"miou\", \"type\": \"quantitative\"}, {\"field\": \"note\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 8]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": null, \"field\": \"miou\", \"scale\": {\"domain\": [60.0, 90.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mean IoU (PASCAL VOC 2012 Test-dev)\"], \"width\": 800}], \"data\": {\"name\": \"data-dda010e8d60d29165aa3390e03ec2404\"}, \"resolve\": {\"scale\": {\"y\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-dda010e8d60d29165aa3390e03ec2404\": [{\"idx\": 1, \"type\": \"Semantic Segmentation\", \"year\": 2015, \"nickname\": \"FCN\", \"miou\": 62.2, \"note\": \"VGG16\"}, {\"idx\": 2, \"type\": \"Semantic Segmentation\", \"year\": 2015, \"nickname\": \"DeconvNet\", \"miou\": 72.5, \"note\": \"VGG16\"}, {\"idx\": 3, \"type\": \"Semantic Segmentation\", \"year\": 2016, \"nickname\": \"DeepLab_v2\", \"miou\": 79.7, \"note\": \"ResNet-101, CRF\"}, {\"idx\": 4, \"type\": \"Semantic Segmentation\", \"year\": 2017, \"nickname\": \"PSPNet\", \"miou\": 85.4, \"note\": \"COCO-pretrained ResNet-101\"}, {\"idx\": 5, \"type\": \"Semantic Segmentation\", \"year\": 2017, \"nickname\": \"DeepLab_v3\", \"miou\": 85.7, \"note\": \"COCO-pretrained ResNet-101\"}, {\"idx\": 6, \"type\": \"Semantic Segmentation\", \"year\": 2018, \"nickname\": \"EncNet\", \"miou\": 85.9, \"note\": \"COCO-pretrained ResNet-101\"}, {\"idx\": 7, \"type\": \"Semantic Segmentation\", \"year\": 2018, \"nickname\": \"DeepLab_v3+\", \"miou\": 89.0, \"note\": \"JFT pretrained Xception-65\"}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.LayerChart(...)"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"semantic_trend_pascal.csv\", sep=\",\")\n",
        "\n",
        "seg_data = data.loc[data['type'] ==\"Semantic Segmentation\"]\n",
        "\n",
        "perf_measure = 'miou'\n",
        "\n",
        "x_scale = [0,data.shape[0]+1]\n",
        "y_scale = [((data[perf_measure].min()//5))*5,((data[perf_measure].max()//5)+1)*5]\n",
        "\n",
        "chart_title = \"Trend on mean IoU (PASCAL VOC 2012 Test-dev)\"\n",
        "\n",
        "base, line, points, point_nick, point_perf = GetGraphElement(chart_title, data, x_scale, y_scale, perf_measure, \n",
        "                                                            line_color = \"#fde725\", point_color = \"#000000\", text_color = \"#000000\", \n",
        "                                                            text_y_pos = -10, textperf_y_pos=20)\n",
        "(\n",
        "    line+points+point_nick+point_perf\n",
        ").resolve_scale(y = 'independent')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "tags": [
          "remove-input"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-d5d163971c704f0d992f44f282c2ec9f\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-d5d163971c704f0d992f44f282c2ec9f\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-d5d163971c704f0d992f44f282c2ec9f\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"line\", \"color\": \"#3b518b\", \"strokeWidth\": 1.5}, \"encoding\": {\"color\": {\"field\": \"type\", \"type\": \"nominal\"}, \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 7]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": {\"grid\": true}, \"field\": \"miou\", \"scale\": {\"domain\": [65.0, 85.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mean IoU (Cityscapes test)\"], \"width\": 800}, {\"mark\": {\"type\": \"circle\", \"color\": \"#000000\", \"strokeWidth\": 3}, \"encoding\": {\"tooltip\": [{\"field\": \"year\", \"type\": \"quantitative\"}, {\"field\": \"nickname\", \"type\": \"nominal\"}, {\"field\": \"miou\", \"type\": \"quantitative\"}, {\"field\": \"note\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 7]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": null, \"field\": \"miou\", \"scale\": {\"domain\": [65.0, 85.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mean IoU (Cityscapes test)\"], \"width\": 800}, {\"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"dy\": -10}, \"encoding\": {\"color\": {\"value\": \"#000000\"}, \"text\": {\"field\": \"miou\", \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"year\", \"type\": \"quantitative\"}, {\"field\": \"nickname\", \"type\": \"nominal\"}, {\"field\": \"miou\", \"type\": \"quantitative\"}, {\"field\": \"note\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 7]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": null, \"field\": \"miou\", \"scale\": {\"domain\": [65.0, 85.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mean IoU (Cityscapes test)\"], \"width\": 800}, {\"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"dy\": 20}, \"encoding\": {\"color\": {\"value\": \"#000000\"}, \"text\": {\"field\": \"nickname\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"year\", \"type\": \"quantitative\"}, {\"field\": \"nickname\", \"type\": \"nominal\"}, {\"field\": \"miou\", \"type\": \"quantitative\"}, {\"field\": \"note\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"idx\", \"scale\": {\"domain\": [0, 7]}, \"type\": \"quantitative\"}, \"y\": {\"axis\": null, \"field\": \"miou\", \"scale\": {\"domain\": [65.0, 85.0]}, \"type\": \"quantitative\"}}, \"title\": [\"Trend on mean IoU (Cityscapes test)\"], \"width\": 800}], \"data\": {\"name\": \"data-60d60ab3b38d264156672d9a21c9b480\"}, \"resolve\": {\"scale\": {\"y\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-60d60ab3b38d264156672d9a21c9b480\": [{\"idx\": 1, \"type\": \"Semantic Segmentation\", \"year\": 2015, \"nickname\": \"FCN\", \"miou\": 65.3, \"note\": \"VGG16\"}, {\"idx\": 2, \"type\": \"Semantic Segmentation\", \"year\": 2016, \"nickname\": \"DeepLab_v2\", \"miou\": 70.4, \"note\": \"ResNet-101, CRF\"}, {\"idx\": 3, \"type\": \"Semantic Segmentation\", \"year\": 2017, \"nickname\": \"PSPNet\", \"miou\": 78.4, \"note\": \"COCO-pretrained ResNet-101\"}, {\"idx\": 4, \"type\": \"Semantic Segmentation\", \"year\": 2017, \"nickname\": \"DeepLab_v3\", \"miou\": 81.3, \"note\": \"COCO-pretrained ResNet-101\"}, {\"idx\": 5, \"type\": \"Semantic Segmentation\", \"year\": 2021, \"nickname\": \"SETR\", \"miou\": 81.6, \"note\": \"SETR-PUP++\"}, {\"idx\": 6, \"type\": \"Semantic Segmentation\", \"year\": 2021, \"nickname\": \"SegFormer\", \"miou\": 83.1, \"note\": \"MiT-B5, Mapillary\"}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.LayerChart(...)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"semantic_trend_cityscape.csv\", sep=\",\")\n",
        "\n",
        "seg_data = data.loc[data['type'] ==\"Semantic Segmentation\"]\n",
        "\n",
        "perf_measure = 'miou'\n",
        "\n",
        "x_scale = [0,data.shape[0]+1]\n",
        "y_scale = [((data[perf_measure].min()//5))*5,((data[perf_measure].max()//5)+1)*5]\n",
        "\n",
        "chart_title = \"Trend on mean IoU (Cityscapes test)\"\n",
        "\n",
        "base, line, points, point_nick, point_perf = GetGraphElement(chart_title, data, x_scale, y_scale, perf_measure, \n",
        "                                                            line_color = \"#3b518b\", point_color = \"#000000\", text_color = \"#000000\", \n",
        "                                                            text_y_pos = -10, textperf_y_pos=20)\n",
        "(\n",
        "    line+points+point_nick+point_perf\n",
        ").resolve_scale(y = 'independent')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- PASCAL VOC 2012는 20개의 object classes와 background class로 구성되어 있으며,  pixel단위의 labed이 존재하는 train DB 1,464장, validation 1,449장 그리고 test 1,456 장으로 구성되어 있다. \n",
        "  \n",
        "+++\n",
        "\n",
        "- 본 chapter에서는 semantic segmentation 논문을 i) fully convolutional network 기반 방법과 ii) convolutional encoder-decoder 방법으로 구분하여 리뷰하고 각 지금까지의 발전 현황과 개선방안에 대해 고찰한다. \n",
        "\n",
        "*Latest update: Nov. 9, 2022*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Panoptic_Segmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
