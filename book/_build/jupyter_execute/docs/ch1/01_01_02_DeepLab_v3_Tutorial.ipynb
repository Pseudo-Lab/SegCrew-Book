{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jerogar/DeepLabV3p-Pytorch-Tutorial_KOR/blob/master/DeepLab_v3_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wi0egK65w35f"
   },
   "source": [
    "# DeepLab v3 Tutorial\r\n",
    "\r\n",
    "가짜연구소 Season 2 논문미식회\r\n",
    "\r\n",
    "reference: https://github.com/VainF/DeepLabV3Plus-Pytorch\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "![Main](https://sthalles.github.io/assets/deep_segmentation_network/semantic_segmentation.jpg)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MfKqK5A4v5Z"
   },
   "source": [
    "# 1. 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ1cnSVwXZvj"
   },
   "source": [
    "## (1) Mount Google Drive\r\n",
    "- 구글 드라이브를 연결하고 작업 폴더를 Colab Notebooks/로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NeGYqxh24oZ-",
    "outputId": "8533f30c-8d1e-4edc-e87f-507eb04e5b62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive\n",
      "Mounted at /content/drive\n",
      "MY_GOOGLE_DRIVE_PATH:  /content/drive/My Drive/Colab Notebooks\n",
      "/content/drive/My Drive/Colab Notebooks\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive # import drive from google colab\r\n",
    "\r\n",
    "ROOT = \"/content/drive\"     # default location for the drive\r\n",
    "print(ROOT)                 # print content of ROOT (Optional)\r\n",
    "\r\n",
    "drive.mount(ROOT)           # we mount the google drive at /content/drive\r\n",
    "\r\n",
    "# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\r\n",
    "from os.path import join  \r\n",
    "\r\n",
    "# path to your project on Google Drive\r\n",
    "MY_GOOGLE_DRIVE_PATH = '/content/drive/My Drive/Colab Notebooks'\r\n",
    "\r\n",
    "print(\"MY_GOOGLE_DRIVE_PATH: \", MY_GOOGLE_DRIVE_PATH)\r\n",
    "# In case we haven't created the folder already; we will create a folder in the project path  \r\n",
    "%cd \"{MY_GOOGLE_DRIVE_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLRclgki42mb"
   },
   "source": [
    "## (2) Set Work directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XoRCnewkB77h"
   },
   "outputs": [],
   "source": [
    "!git clone DeepLabV3p-Pytorch-Tutorial_KOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6ohDpwJ95R7"
   },
   "outputs": [],
   "source": [
    "%cd DeepLabV3p-Pytorch-Tutorial_KOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOi091h8cAbI"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision numpy pillow scikit-learn tqdm matplotlib opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_Sbl6YTaxtp"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\r\n",
    "import os\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from torch.utils import data\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "\r\n",
    "import torch.nn.functional as F\r\n",
    "from collections import OrderedDict\r\n",
    "\r\n",
    "from torchvision.transforms.functional import normalize\r\n",
    "from sklearn.metrics import confusion_matrix\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrHnDiOzJ2CB"
   },
   "source": [
    "#2. DeepLab v3 Model\r\n",
    "\r\n",
    "- DeepLab v1 - atrous convolution 및 Conditional Random Field(CRF) 도입\r\n",
    "- DeepLab v2 - A*trous spatial pyramid pooling* (ASPP) 이용하여 다양한 크기의 객체에 대응\r\n",
    "- DeepLab v3 - encoder에 ResNet 구조 도입. ASPP를 보완하고, batch norm. 사용하여 학습이 잘 되도록 함.  CRF 없이 동등 이상 성능 확보\r\n",
    "- DeepLab v3+: Depth-wise seperable Conv.도입하여 런타임 속도 개선. (ASSPP) U-Net 구조를 단순화한 Decoder 구조 사용하여 성능 개선\r\n",
    "\r\n",
    "![deeplab](https://miro.medium.com/max/1038/0*_Hm_2fqbcnlwLkoz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zacdJ4PLmAM-"
   },
   "source": [
    "## (1) Overall Structure\r\n",
    "![deeplabv3](https://www.oreilly.com/library/view/hands-on-image-processing/9781789343731/assets/1aa5b349-5a66-456a-8afa-080a7b07a525.png)\r\n",
    "\r\n",
    "- Backbone Network + DeepLab Head 구조\r\n",
    "- Resnet의 layer 4의 feature를 backbone feature로 사용\r\n",
    "- output_stride를 8로 설정하는 경우 \r\n",
    "    - Backborn의 stride=8에 해당하는 layer까지 사용\r\n",
    "    - ResNet을 사용하는 경우 layer2까지 사용하며, stride를 8로 유지하며 layer 3, 4의 atrous  convolution으로 대체.\r\n",
    "    - DeepLab Head의 Atrous Spatial Pyramid Pooling(ASPP)의 dilation rate=[12, 24, 36] 사용\r\n",
    "- output_stride를 16로 설정하는 경우 \r\n",
    "    - Backborn의 stride=16에 해당하는 layer까지 사용\r\n",
    "    - ResNet을 사용하는 경우 layer3까지 사용하며, stride를 16로 유지하며 layer 4를 atrous  convolution으로 대체.\r\n",
    "    - DeepLab Head의 Atrous Spatial Pyramid Pooling(ASPP)의 dilation rate=[6, 12, 18] 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyzYYks05t9D"
   },
   "outputs": [],
   "source": [
    "from network.backbone import resnet\r\n",
    "from network import utils\r\n",
    "from collections import OrderedDict\r\n",
    "\r\n",
    "def ResNetbasedDeepLabV3(num_classes, output_stride, pretrained_backbone):\r\n",
    "    if output_stride==8:\r\n",
    "        replace_stride_with_dilation=[False, True, True]\r\n",
    "        aspp_dilate = [12, 24, 36]\r\n",
    "    else:\r\n",
    "        replace_stride_with_dilation=[False, False, True]\r\n",
    "        aspp_dilate = [6, 12, 18]\r\n",
    "        \r\n",
    "    backbone = resnet.resnet50(\r\n",
    "        pretrained=pretrained_backbone,\r\n",
    "        replace_stride_with_dilation=replace_stride_with_dilation)\r\n",
    "    \r\n",
    "    inplanes = 2048\r\n",
    "    low_level_planes = 256\r\n",
    "\r\n",
    "    return_layers = {'layer4': 'out'}\r\n",
    "    classifier = DeepLabHead(inplanes , num_classes, aspp_dilate)\r\n",
    "    backbone = utils.IntermediateLayerGetter(backbone, return_layers=return_layers)\r\n",
    "\r\n",
    "\r\n",
    "    # TODO: interpolation parameter 추가\r\n",
    "    model = DeepLabV3(backbone, classifier, 'bilinear')\r\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7o63GzStmvhK"
   },
   "source": [
    "##(3) DeepLab Head\r\n",
    "- ASPP(2048ch→ 256ch) + 3x3 Conv(256ch→256ch) + 1x1 Conv(256ch→21:number of class)\r\n",
    "\r\n",
    "- Backbone + DeepLab v3 Head를 거쳐 도출된 segmentation 결과를, Bilinear upsampling을 이용하여 image-level 해상도로 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9vmrVu7nv2W"
   },
   "outputs": [],
   "source": [
    "class DeepLabV3(nn.Module):\r\n",
    "\r\n",
    "    # TODO: interpolation parameter 추가\r\n",
    "    #def __init__(self, backbone, classifier):\r\n",
    "    def __init__(self, backbone, classifier, upMethod = ''):\r\n",
    "        super(DeepLabV3, self).__init__()\r\n",
    "        self.backbone = backbone\r\n",
    "        self.classifier = classifier\r\n",
    "        self.upsampleMethod = upMethod\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        input_shape = x.shape[-2:]\r\n",
    "        input_ch = x.shape[1] # 256\r\n",
    "        features = self.backbone(x)\r\n",
    "        x = self.classifier(features)\r\n",
    "\r\n",
    "        if self.upsampleMethod == 'CARAFE':\r\n",
    "            # upScale =input_shape[1] / x.shape[-1]\r\n",
    "            # print(input_ch)\r\n",
    "\r\n",
    "            carafe =CARAFE(c=21, scale=2)\r\n",
    "            if torch.cuda.is_available():\r\n",
    "                carafe.cuda()\r\n",
    "            x = carafe(x) \r\n",
    "            \r\n",
    "            x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\r\n",
    "\r\n",
    "        else :\r\n",
    "            x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\r\n",
    "        \r\n",
    "        return x\r\n",
    "\r\n",
    "class DeepLabHead(nn.Module):\r\n",
    "    def __init__(self, in_channels, num_classes, aspp_dilate=[12, 24, 36]):\r\n",
    "        super(DeepLabHead, self).__init__()\r\n",
    "\r\n",
    "        self.classifier = nn.Sequential(\r\n",
    "            ASPP(in_channels, aspp_dilate),\r\n",
    "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\r\n",
    "            nn.BatchNorm2d(256),\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(256, num_classes, 1)\r\n",
    "        )\r\n",
    "        self._init_weight()\r\n",
    "\r\n",
    "    def forward(self, feature):\r\n",
    "        return self.classifier( feature['out'] )\r\n",
    "\r\n",
    "    def _init_weight(self):\r\n",
    "        for m in self.modules():\r\n",
    "            if isinstance(m, nn.Conv2d):\r\n",
    "                nn.init.kaiming_normal_(m.weight)\r\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\r\n",
    "                nn.init.constant_(m.weight, 1)\r\n",
    "                nn.init.constant_(m.bias, 0)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7LS-HGQVQou"
   },
   "source": [
    "### ASPP (Atrous Spatial Pyramid Pooling)\r\n",
    "- 초기 Object Detection에 적용된 SPPNet의 Idea 차용 Atrous Conv.로 구성한 SPP 구성\r\n",
    "\r\n",
    "    → ConvLayer의 output을 다양한 Atrous rate를 가지는 kernel을 병렬로 연산하여 multi-scale 특징을 추출하고자 함.\r\n",
    "\r\n",
    "- Atrous rate가 커질수록 유효한 weight의 수가 작아지는 경향을 보임.\r\n",
    "    ⇒  Atorus rate가 커지면 3x3 kernel이 1x1처럼 동작함.\r\n",
    "\r\n",
    "    ⇒ large scale context는 output에 반영 안됨. (receptive field를 크게 만들고 싶은데 안됨..)\r\n",
    "\r\n",
    "- 이런 degenerate 문제 해결을 위해 이미지의 last feature map에 대해서 \"Global average pooling\"을 적용.\r\n",
    "- 성능 개선을 위해 각 module에 batch normalization 적용\r\n",
    "\r\n",
    "![ASPP](https://gaussian37.github.io/assets/img/vision/segmentation/aspp/0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4pFO0OjUTQB"
   },
   "source": [
    "### Atrous Convolution (=Dilated Convolution)\r\n",
    "\r\n",
    "- Kernel의 성분들 사이에 빈 성분(0)을 삽입하여 convolution을 수행\r\n",
    "- Dilation rate는 빈 성분(0)을 몇 개 삽입할지 결정. 기존 일반적인 convolution의 dilation rate=1. dilation rate=2일 경우는 성분들 사이에 빈 성분(0) 1개 추가, 3일 경우는 2개 추가, ...\r\n",
    "\r\n",
    "- 기존 3x3 Kernel에 아래와 같이 dilation rate=2를 사용하면 field-of-view(receptive field)가 5x5의 영역을 커버하게 됨. \r\n",
    "\r\n",
    "- x: input feature map, w: filter, r: dilation rate, y: output일 때 다음과 같이 표현 가능하다. \r\n",
    "$$y[i]=\\sum_{k}^{K}x[i+r\\cdot{k}]w[k]$$\r\n",
    "\r\n",
    "![https://miro.medium.com/max/395/1*1okwhewf5KCtIPaFib4XaA.gif](https://miro.medium.com/max/395/1*1okwhewf5KCtIPaFib4XaA.gif)\r\n",
    "*convolution*\r\n",
    "|\r\n",
    "![https://miro.medium.com/max/395/1*SVkgHoFoiMZkjy54zM_SUw.gif](https://miro.medium.com/max/395/1*SVkgHoFoiMZkjy54zM_SUw.gif)*Atrous Conv.*\r\n",
    "\r\n",
    "- **Atrous convolution은 pooling 연산 없이 넓은** field of view를 커버할 수 있음.\r\n",
    "(기존 CNN의 경우 넓은 field of view를 커버하기 위해 conv. pooling을 사용하여 output feature map의 해상도가 감소.)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48poPshQUSPz"
   },
   "outputs": [],
   "source": [
    "class ASPPConv(nn.Sequential):\r\n",
    "    def __init__(self, in_channels, out_channels, dilation):\r\n",
    "        modules = [\r\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\r\n",
    "            nn.BatchNorm2d(out_channels),\r\n",
    "            nn.ReLU(inplace=True)\r\n",
    "        ]\r\n",
    "        super(ASPPConv, self).__init__(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjdyGj9tncj-"
   },
   "source": [
    "- Global Average Pooling + 1x1 Conv + Upsampling\r\n",
    "    - Global Average Pooling (nn.AdaptiveAvgPool2d): Channel별로 average를 구하여 pooling (ex. 2048x33x33 → 2048x1x1)\r\n",
    "    - 1x1 Conv: Output channel 수를 256으로  (ex. 2048x1x1 → 256x1x1)\r\n",
    "    - Upsampling: Size를 원래대로 (ex. 256x1x1 → 256x33x33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcFlb6_gnNnM"
   },
   "outputs": [],
   "source": [
    "class ASPPPooling(nn.Sequential):\r\n",
    "    def __init__(self, in_channels, out_channels):\r\n",
    "        super(ASPPPooling, self).__init__(\r\n",
    "            nn.AdaptiveAvgPool2d(1),\r\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n",
    "            nn.BatchNorm2d(out_channels),\r\n",
    "            nn.ReLU(inplace=True))\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        size = x.shape[-2:]\r\n",
    "        x = super(ASPPPooling, self).forward(x)\r\n",
    "        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gvqtb5ZXnTpE"
   },
   "source": [
    "- 병렬화된 1x1 Conv, 3개의 3x3 Dilated Conv, Image pooling(code의 ASPP Pooling)를 concatenate\r\n",
    "- Concatenate된 feature를 1x1 Conv를 통해 output channel을 256으로 생성 (5x256→256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pumb6ECinPG_"
   },
   "outputs": [],
   "source": [
    "class ASPP(nn.Module):\r\n",
    "    def __init__(self, in_channels, atrous_rates):\r\n",
    "        super(ASPP, self).__init__()\r\n",
    "        out_channels = 256\r\n",
    "        modules = []\r\n",
    "        modules.append(nn.Sequential(\r\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n",
    "            nn.BatchNorm2d(out_channels),\r\n",
    "            nn.ReLU(inplace=True)))\r\n",
    "\r\n",
    "        rate1, rate2, rate3 = tuple(atrous_rates)\r\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate1))\r\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate2))\r\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate3))\r\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\r\n",
    "\r\n",
    "        self.convs = nn.ModuleList(modules)\r\n",
    "\r\n",
    "        self.project = nn.Sequential(\r\n",
    "            nn.Conv2d(5 * out_channels, out_channels, 1, bias=False),\r\n",
    "            nn.BatchNorm2d(out_channels),\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Dropout(0.1),)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        res = []\r\n",
    "        for conv in self.convs:\r\n",
    "            res.append(conv(x))\r\n",
    "        res = torch.cat(res, dim=1)\r\n",
    "        return self.project(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYfdrVT43EPS"
   },
   "source": [
    "##(4) CARAFE\n",
    "Code from: https://github.com/XiaLiPKU/CARAFE/blob/master/carafe.py#L68\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnk8vvQN4xVI"
   },
   "outputs": [],
   "source": [
    "class ConvBNReLU(nn.Module):\n",
    "    '''Module for the Conv-BN-ReLU tuple.'''\n",
    "    def __init__(self, c_in, c_out, kernel_size, stride, padding, dilation,\n",
    "                 use_relu=True):\n",
    "        super(ConvBNReLU, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "                c_in, c_out, kernel_size=kernel_size, stride=stride, \n",
    "                padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c_out)\n",
    "        if use_relu:\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            self.relu = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CARAFE(nn.Module):\n",
    "    def __init__(self, c, c_mid=64, scale=2, k_up=5, k_enc=3):\n",
    "        \"\"\" The unofficial implementation of the CARAFE module.\n",
    "        The details are in \"https://arxiv.org/abs/1905.02188\".\n",
    "        Args:\n",
    "            c: The channel number of the input and the output.\n",
    "            c_mid: The channel number after compression.\n",
    "            scale: The expected upsample scale.\n",
    "            k_up: The size of the reassembly kernel.\n",
    "            k_enc: The kernel size of the encoder.\n",
    "        Returns:\n",
    "            X: The upsampled feature map.\n",
    "        \"\"\"\n",
    "        super(CARAFE, self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "        self.comp = ConvBNReLU(c, c_mid, kernel_size=1, stride=1, \n",
    "                               padding=0, dilation=1)\n",
    "        self.enc = ConvBNReLU(c_mid, (scale*k_up)**2, kernel_size=k_enc, \n",
    "                              stride=1, padding=k_enc//2, dilation=1, \n",
    "                              use_relu=False)\n",
    "        self.pix_shf = nn.PixelShuffle(scale)\n",
    "\n",
    "        self.upsmp = nn.Upsample(scale_factor=scale, mode='nearest')\n",
    "        self.unfold = nn.Unfold(kernel_size=k_up, dilation=scale, \n",
    "                                padding=k_up//2*scale)\n",
    "\n",
    "    def forward(self, X):\n",
    "        b, c, h, w = X.size()\n",
    "        h_, w_ = h * self.scale, w * self.scale\n",
    "        \n",
    "        W = self.comp(X)                                # b * m * h * w\n",
    "        W = self.enc(W)                                 # b * 100(sigma^2, k_up^2) * h * w\n",
    "        W = self.pix_shf(W)                             # b * 25(k_up^2) * h_ * w_\n",
    "        W = F.softmax(W, dim=1)                         # b * 25 * h_ * w_\n",
    "\n",
    "        X = self.upsmp(X)                               # b * c * h_ * w_\n",
    "        X = self.unfold(X)                              # b * 25c * h_ * w_\n",
    "        X = X.view(b, c, -1, h_, w_)                    # b * 25 * c * h_ * w_\n",
    "\n",
    "        X = torch.einsum('bkhw,bckhw->bchw', [W, X])    # b * c * h_ * w_\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3vgfeEzqEeC"
   },
   "source": [
    "# 3. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMUH2XvqcTJ8"
   },
   "source": [
    "###(1) PASCAL VOC 2012 Segmentation Task Dataset\r\n",
    "\r\n",
    "http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\r\n",
    "https://academictorrents.com/details/df0aad374e63b3214ef9e92e178580ce27570e59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJFZZQFc2yDz"
   },
   "outputs": [],
   "source": [
    "!wget -P datasets/data/ \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\r\n",
    "!tar -xvf datasets/data/VOCtrainval_11-May-2012.tar -C ./datasets/data\r\n",
    "\r\n",
    "# SegmentationClassAug.zip를 다운. (https://www.dropbox.com/s/oeu149j8qtbs1x0/SegmentationClassAug.zip?dl=0)\r\n",
    "!wget -P datasets/data/ \"https://www.dropbox.com/s/oeu149j8qtbs1x0/SegmentationClassAug.zip?dl=1\"\r\n",
    "!unzip datasets/data/SegmentationClassAug.zip -d datasets/data/VOCdevkit/VOC2012/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htMjRT9LccCX"
   },
   "source": [
    "###(2) Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7MlH7HVl34i"
   },
   "source": [
    "- Scaling, Crop, 상하반전으로 Training data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-Zogp0wa3nX"
   },
   "outputs": [],
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "from datasets import VOCSegmentation, Cityscapes\r\n",
    "from utils import ext_transforms as et\r\n",
    "\r\n",
    "def get_dataset(opts):\r\n",
    "    \"\"\" Dataset And Augmentation\r\n",
    "    \"\"\"\r\n",
    "    if opts.dataset == 'voc':\r\n",
    "        train_transform = et.ExtCompose([\r\n",
    "            #et.ExtResize(size=opts.crop_size),\r\n",
    "            et.ExtRandomScale((0.5, 2.0)),\r\n",
    "            et.ExtRandomCrop(size=(opts.crop_size, opts.crop_size), pad_if_needed=True),\r\n",
    "            et.ExtRandomHorizontalFlip(),\r\n",
    "            et.ExtToTensor(),\r\n",
    "            et.ExtNormalize(mean=[0.485, 0.456, 0.406],\r\n",
    "                            std=[0.229, 0.224, 0.225]),\r\n",
    "        ])\r\n",
    "        if opts.crop_val:\r\n",
    "            val_transform = et.ExtCompose([\r\n",
    "                et.ExtResize(opts.crop_size),\r\n",
    "                et.ExtCenterCrop(opts.crop_size),\r\n",
    "                et.ExtToTensor(),\r\n",
    "                et.ExtNormalize(mean=[0.485, 0.456, 0.406],\r\n",
    "                                std=[0.229, 0.224, 0.225]),\r\n",
    "            ])\r\n",
    "        else:\r\n",
    "            val_transform = et.ExtCompose([\r\n",
    "                et.ExtToTensor(),\r\n",
    "                et.ExtNormalize(mean=[0.485, 0.456, 0.406],\r\n",
    "                                std=[0.229, 0.224, 0.225]),\r\n",
    "            ])\r\n",
    "        train_dst = VOCSegmentation(root=opts.data_root, year=opts.year,\r\n",
    "                                    image_set='train', transform=train_transform)\r\n",
    "        val_dst = VOCSegmentation(root=opts.data_root, year=opts.year,\r\n",
    "                                  image_set='val', transform=val_transform)\r\n",
    "    \r\n",
    "    return train_dst, val_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3paRH68cENLS"
   },
   "source": [
    "#4. Train/Test\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVkLeySIrfQU"
   },
   "source": [
    "### (1) poly learning rate policy\r\n",
    "\r\n",
    "- 초반엔 선형 감소에 가깝지만, Training step의 마지막에서는 조금 더 가파르게 감소하는 경향성을 나타냄\r\n",
    "$$learning\\_rate = \\left( 1-\\frac{iter}{max\\_iter}\\right)^{power}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8F-BsVwkMyGS"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler, StepLR\r\n",
    "\r\n",
    "class PolyLR(_LRScheduler):\r\n",
    "    def __init__(self, optimizer, max_iters, power=0.9, last_epoch=-1, min_lr=1e-6):\r\n",
    "        self.power = power\r\n",
    "        self.max_iters = max_iters  # avoid zero lr\r\n",
    "        self.min_lr = min_lr\r\n",
    "        super(PolyLR, self).__init__(optimizer, last_epoch)\r\n",
    "    \r\n",
    "    def get_lr(self):\r\n",
    "        return [ max( base_lr * ( 1 - self.last_epoch/self.max_iters )**self.power, self.min_lr)\r\n",
    "                for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxOD7cMcrpVm"
   },
   "source": [
    "###(2) Focal Loss\r\n",
    "- ignore_index: 특정 target value를 무시하며, gradient 계산 할 때 포함시키지 않는다. size_average가 true일 경우 ignore_index되어 있는 target을 제외하고 평균을 계산한다.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qO6caE8yM1fy"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch \r\n",
    "\r\n",
    "class FocalLoss(nn.Module):\r\n",
    "    def __init__(self, alpha=1, gamma=0, size_average=True, ignore_index=255):\r\n",
    "        super(FocalLoss, self).__init__()\r\n",
    "        self.alpha = alpha\r\n",
    "        self.gamma = gamma\r\n",
    "        self.ignore_index = ignore_index\r\n",
    "        self.size_average = size_average\r\n",
    "\r\n",
    "    def forward(self, inputs, targets):\r\n",
    "        ce_loss = F.cross_entropy(\r\n",
    "            inputs, targets, reduction='none', ignore_index=self.ignore_index)\r\n",
    "        pt = torch.exp(-ce_loss)\r\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\r\n",
    "        if self.size_average:\r\n",
    "            return focal_loss.mean()\r\n",
    "        else:\r\n",
    "            return focal_loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lY9cLh7vKE48"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "\n",
    "def set_bn_momentum(model, momentum=0.1):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            m.momentum = momentum\n",
    "\n",
    "def save_ckpt(path):\n",
    "    \"\"\" save current model\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        \"cur_itrs\": cur_itrs,\n",
    "        \"model_state\": model.module.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"best_score\": best_score,\n",
    "    }, path)\n",
    "    print(\"Model saved as %s\" % path)\n",
    "\n",
    "class Denormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        mean = np.array(mean)\n",
    "        std = np.array(std)\n",
    "        self._mean = -mean/std\n",
    "        self._std = 1/std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        if isinstance(tensor, np.ndarray):\n",
    "            return (tensor - self._mean.reshape(-1,1,1)) / self._std.reshape(-1,1,1)\n",
    "        return normalize(tensor, self._mean, self._std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aE4DxXTPizUV"
   },
   "source": [
    "###(3) Validation / Performance Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRTzZ96Ma6Yl"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "from PIL import Image\r\n",
    "\r\n",
    "import cv2\r\n",
    "import matplotlib\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "\r\n",
    "def validate(opts, model, loader, device, metrics):\r\n",
    "    \"\"\"Do validation and return specified samples\"\"\"\r\n",
    "    metrics.reset()\r\n",
    "    ret_samples = []\r\n",
    "    if opts.save_val_results:\r\n",
    "        if not os.path.exists('results'):\r\n",
    "            os.mkdir('results')\r\n",
    "        denorm = Denormalize(mean=[0.485, 0.456, 0.406], \r\n",
    "                              std=[0.229, 0.224, 0.225])\r\n",
    "        img_id = 0\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for i, (images, labels) in tqdm(enumerate(loader)):\r\n",
    "            \r\n",
    "            images = images.to(device, dtype=torch.float32)\r\n",
    "            labels = labels.to(device, dtype=torch.long)\r\n",
    "\r\n",
    "            outputs = model(images)\r\n",
    "            preds = outputs.detach().max(dim=1)[1].cpu().numpy()\r\n",
    "            targets = labels.cpu().numpy()\r\n",
    "\r\n",
    "            metrics.update(targets, preds)\r\n",
    "\r\n",
    "            if opts.save_val_results:\r\n",
    "                for i in range(len(images)):\r\n",
    "                    image = images[i].detach().cpu().numpy()\r\n",
    "                    target = targets[i]\r\n",
    "                    pred = preds[i]\r\n",
    "\r\n",
    "                    image = (denorm(image) * 255).transpose(1, 2, 0).astype(np.uint8)\r\n",
    "                    target = loader.dataset.decode_target(target).astype(np.uint8)\r\n",
    "                    pred = loader.dataset.decode_target(pred).astype(np.uint8)\r\n",
    "\r\n",
    "                    Image.fromarray(image).save('results/%d_image.png' % img_id)\r\n",
    "                    Image.fromarray(target).save('results/%d_target.png' % img_id)\r\n",
    "                    Image.fromarray(pred).save('results/%d_pred.png' % img_id)\r\n",
    "\r\n",
    "                    fig = plt.figure()\r\n",
    "                    plt.ion()\r\n",
    "                    plt.imshow(image)\r\n",
    "                    plt.axis('off')\r\n",
    "                    plt.imshow(pred, alpha=0.7)\r\n",
    "                    ax = plt.gca()\r\n",
    "                    ax.xaxis.set_major_locator(matplotlib.ticker.NullLocator())\r\n",
    "                    ax.yaxis.set_major_locator(matplotlib.ticker.NullLocator())\r\n",
    "                    plt.savefig('results/%d_overlay.png' % img_id, bbox_inches='tight', pad_inches=0)\r\n",
    "                    plt.close()\r\n",
    "\r\n",
    "                    #fig2 = plt.figure()\r\n",
    "                    if opts.test_only:\r\n",
    "                      imgBGR = cv2.imread('results/%d_image.png' % img_id)\r\n",
    "\r\n",
    "                      # cv2.imread는 BGR로 불러오므로 plt를 이용하려면 RGB로 바꿔줘야 함\r\n",
    "                      imgTargetBGR = cv2.imread('results/%d_target.png' % img_id)\r\n",
    "                      imgPredBGR = cv2.imread('results/%d_pred.png' % img_id)\r\n",
    "                      imgOverBGR = cv2.imread('results/%d_overlay.png' % img_id)\r\n",
    "                      \r\n",
    "                      fig2 = plt.figure()\r\n",
    "                      plt.ion()\r\n",
    "                      ax = fig2.add_subplot(1,4,1)\r\n",
    "                      plt.axis('off')\r\n",
    "                      imgplot = plt.imshow(imgBGR)\r\n",
    "                      ax.set_title('input')\r\n",
    "                      ax = fig2.add_subplot(1,4,2)\r\n",
    "                      plt.axis('off')\r\n",
    "                      imgplot = plt.imshow(imgTargetBGR)\r\n",
    "                      ax.set_title('GT')\r\n",
    "                      ax = fig2.add_subplot(1,4,3)\r\n",
    "                      plt.axis('off')\r\n",
    "                      imgplot = plt.imshow(imgPredBGR)\r\n",
    "                      ax.set_title('Seg.Mask')\r\n",
    "                      ax = fig2.add_subplot(1,4,4)\r\n",
    "                      plt.axis('off')\r\n",
    "                      imgplot = plt.imshow(imgOverBGR)\r\n",
    "                      ax.set_title('input with mask')                        \r\n",
    "                      plt.show()\r\n",
    "                      plt.close()\r\n",
    "\r\n",
    "                    img_id += 1\r\n",
    "\r\n",
    "        score = metrics.get_results()\r\n",
    "    return score, ret_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnPKZT2Wr_Th"
   },
   "source": [
    "##(4) Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0W4FAzZoeLGB"
   },
   "outputs": [],
   "source": [
    "class Config():\r\n",
    "    # TODO : ADD compound scaling factor\r\n",
    "    def __init__(self):\r\n",
    "        super(Config, self).__init__()\r\n",
    "\r\n",
    "        self.data_root='./datasets/data'\r\n",
    "        self.dataset ='voc'\r\n",
    "        self.download =False\r\n",
    "        self.year ='2012' # VOC\r\n",
    "\r\n",
    "        self.num_classes = 21\r\n",
    "        self.model ='deeplabv3_resnet50_CARAFE'\r\n",
    "        self.output_stride =16\r\n",
    "\r\n",
    "        self.pretrained_backbone = True\r\n",
    "\r\n",
    "        # train option\r\n",
    "        self.test_only =False\r\n",
    "        self.save_val_results =True\r\n",
    "        self.total_itrs =30e3\r\n",
    "        self.lr =0.01\r\n",
    "        self.lr_policy ='poly' # 'step'\r\n",
    "        self.step_size =10000\r\n",
    "        self.crop_val =False\r\n",
    "        self.batch_size =16\r\n",
    "        self.val_batch_size =4\r\n",
    "        self.crop_size =513\r\n",
    "        self.ckpt ='checkpoints/latest_deeplabv3_resnet50_CARAFE_voc_os16.pth'\r\n",
    "        self.continue_training =False\r\n",
    "\r\n",
    "        self.loss_type = 'cross_entropy' #'cross_entropy', 'focal_loss'\r\n",
    "        self.gpu_id ='0'\r\n",
    "        self.weight_decay =1e-4\r\n",
    "        self.random_seed =1\r\n",
    "        self.print_interval =10\r\n",
    "        self.val_interval =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JORB0j-csjm"
   },
   "outputs": [],
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch.utils import data\r\n",
    "\r\n",
    "from metrics import StreamSegMetrics\r\n",
    "\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "\r\n",
    "# Load option\r\n",
    "opts =Config()\r\n",
    "\r\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = opts.gpu_id\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "print(\"Device: %s\" % device)\r\n",
    "\r\n",
    "# Setup random seed\r\n",
    "torch.manual_seed(opts.random_seed)\r\n",
    "np.random.seed(opts.random_seed)\r\n",
    "random.seed(opts.random_seed)\r\n",
    "\r\n",
    "\r\n",
    "# Set dataloader\r\n",
    "if opts.dataset.lower() == 'voc':\r\n",
    "    opts.num_classes = 21\r\n",
    "#elif opts.dataset.lower() == 'cityscapes':\r\n",
    "#    opts.num_classes = 19\r\n",
    "\r\n",
    "if opts.dataset=='voc' and not opts.crop_val:\r\n",
    "    opts.val_batch_size = 1\r\n",
    "\r\n",
    "train_dst, val_dst = get_dataset(opts)\r\n",
    "train_loader = data.DataLoader(\r\n",
    "    train_dst, batch_size=opts.batch_size, shuffle=True, num_workers=2)\r\n",
    "val_loader = data.DataLoader(\r\n",
    "    val_dst, batch_size=opts.val_batch_size, shuffle=True, num_workers=2)\r\n",
    "print(\"Dataset: %s, Train set: %d, Val set: %d\" %\r\n",
    "      (opts.dataset, len(train_dst), len(val_dst)))\r\n",
    "\r\n",
    "# Set up model\r\n",
    "model  = ResNetbasedDeepLabV3(num_classes=opts.num_classes, output_stride=opts.output_stride, pretrained_backbone=opts.pretrained_backbone)\r\n",
    "set_bn_momentum(model.backbone, momentum=0.01)\r\n",
    "\r\n",
    "# Set up optimizer\r\n",
    "optimizer = torch.optim.SGD(params=[\r\n",
    "    {'params': model.backbone.parameters(), 'lr': 0.1*opts.lr},\r\n",
    "    {'params': model.classifier.parameters(), 'lr': opts.lr},\r\n",
    "], lr=opts.lr, momentum=0.9, weight_decay=opts.weight_decay)\r\n",
    "\r\n",
    "if opts.lr_policy=='poly':\r\n",
    "    scheduler = PolyLR(optimizer, opts.total_itrs, power=0.9)\r\n",
    "elif opts.lr_policy=='step':\r\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=opts.step_size, gamma=0.1)\r\n",
    "\r\n",
    "# Set up criterion\r\n",
    "if opts.loss_type == 'focal_loss':\r\n",
    "    criterion = FocalLoss(ignore_index=255, size_average=True)\r\n",
    "elif opts.loss_type == 'cross_entropy':\r\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=255, reduction='mean')\r\n",
    "\r\n",
    "# Set up metrics\r\n",
    "metrics = StreamSegMetrics(opts.num_classes)\r\n",
    "\r\n",
    "checkPointPath = 'checkpoints'\r\n",
    "if not os.path.exists(checkPointPath):\r\n",
    "    os.mkdir(checkPointPath)\r\n",
    "# Restore\r\n",
    "best_score = 0.0\r\n",
    "cur_itrs = 0\r\n",
    "cur_epochs = 0\r\n",
    "\r\n",
    "if opts.ckpt is not None and os.path.isfile(opts.ckpt):\r\n",
    "    # https://github.com/VainF/DeepLabV3Plus-Pytorch/issues/8#issuecomment-605601402, @PytaichukBohdan\r\n",
    "    checkpoint = torch.load(opts.ckpt, map_location=torch.device('cuda'))\r\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\r\n",
    "    model = nn.DataParallel(model)\r\n",
    "    model.to(device)\r\n",
    "    if opts.continue_training:\r\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\r\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\r\n",
    "        cur_itrs = checkpoint[\"cur_itrs\"]\r\n",
    "        best_score = checkpoint['best_score']\r\n",
    "        print(\"Training state restored from %s\" % opts.ckpt)\r\n",
    "    print(\"Model restored from %s\" % opts.ckpt)\r\n",
    "    del checkpoint  # free memory\r\n",
    "else:\r\n",
    "    print(\"[!] Retrain\")\r\n",
    "    model = nn.DataParallel(model)\r\n",
    "    model.to(device)\r\n",
    "\r\n",
    "denorm = Denormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # denormalization for ori images\r\n",
    "\r\n",
    "if opts.test_only:\r\n",
    "    model.eval()\r\n",
    "    val_score, ret_samples = validate(opts=opts, model=model, loader=val_loader, device=device, metrics=metrics)\r\n",
    "    print(metrics.to_str(val_score))\r\n",
    "\r\n",
    "else:\r\n",
    "  interval_loss = 0\r\n",
    "  while True: #cur_itrs < opts.total_itrs:\r\n",
    "      # =====  Train  =====\r\n",
    "      model.train()\r\n",
    "      cur_epochs += 1\r\n",
    "      for (images, labels) in train_loader:\r\n",
    "          cur_itrs += 1\r\n",
    "\r\n",
    "          images = images.to(device, dtype=torch.float32)\r\n",
    "          labels = labels.to(device, dtype=torch.long)\r\n",
    "\r\n",
    "          optimizer.zero_grad()\r\n",
    "          outputs = model(images)\r\n",
    "          loss = criterion(outputs, labels)\r\n",
    "          loss.backward()\r\n",
    "          optimizer.step()\r\n",
    "\r\n",
    "          np_loss = loss.detach().cpu().numpy()\r\n",
    "          interval_loss += np_loss\r\n",
    "\r\n",
    "          if (cur_itrs) % 10 == 0:\r\n",
    "              interval_loss = interval_loss/10\r\n",
    "              print(\"Epoch %d, Itrs %d/%d, Loss=%f\" %\r\n",
    "                    (cur_epochs, cur_itrs, opts.total_itrs, interval_loss))\r\n",
    "              interval_loss = 0.0\r\n",
    "\r\n",
    "          if (cur_itrs) % opts.val_interval == 0:\r\n",
    "              save_ckpt('checkpoints/latest_%s_%s_os%d.pth' %\r\n",
    "                        (opts.model, opts.dataset, opts.output_stride))\r\n",
    "              \r\n",
    "              print(\"validation...\")\r\n",
    "              \r\n",
    "              model.eval()\r\n",
    "              val_score, ret_samples = validate(\r\n",
    "                  opts=opts, model=model, loader=val_loader, device=device, metrics=metrics)\r\n",
    "              print(metrics.to_str(val_score))\r\n",
    "              if val_score['Mean IoU'] > best_score:  # save best model\r\n",
    "                  best_score = val_score['Mean IoU']\r\n",
    "                  save_ckpt('checkpoints/best_%s_%s_os%d.pth' %\r\n",
    "                            (opts.model, opts.dataset,opts.output_stride))\r\n",
    "\r\n",
    "              model.train()\r\n",
    "          scheduler.step()  \r\n",
    "\r\n",
    "          #if cur_itrs >=  opts.total_itrs:\r\n",
    "              #return\r\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPYl+JiNhZ9axoc/AuCTS6/",
   "collapsed_sections": [
    "N7LS-HGQVQou"
   ],
   "include_colab_link": true,
   "name": "Copy of DeepLab_v3_Tutorial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}