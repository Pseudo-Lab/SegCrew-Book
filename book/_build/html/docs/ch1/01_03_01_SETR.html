
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SETR - CVPR 2021 &#8212; Segmentation Complete</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e8f53015daec13862f6db5e763c41738.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="shortcut icon" href="../../_static/PseudoLab_logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Overview" href="../ch2/02%20Instance%20Segmentation2.html" />
    <link rel="prev" title="C. Transformer based method" href="01_03_transformer.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/PseudoLab_logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Segmentation Complete</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   [가짜연구소] Segmentation 완전정복
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Semantic Segmentation
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_Semantic%20Segmentation2.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="01_01_fullyconvNet.html">
   A. Fully Convolutional Network
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="01_01_01_fcn.html">
     FCN - CVPR 2015
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="01_01_02_DeepLabv3.html">
     DeepLab v3 - arXiv 2017
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="01_01_03_pspnet.html">
     PSPNet - CVPR 2017
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="01_01_04_encnet.html">
     EncNet - CVPR 2018
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="01_02_encodedecode.html">
   B. Convolutional encoder-decoder
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="01_02_01_unet.html">
     U-Net - arxiv 2015
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="01_02_03_DeconvNet.html">
     DeconvNet - ICCV 2015
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="01_02_02_segnet.html">
     SegNet - TPAMI 2017
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="01_02_01_multiresunet.html">
     MultiResUNet - arxiv 2019
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="01_03_transformer.html">
   C. Transformer based method
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     SETR - CVPR 2021
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Instance Segmentation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch2/02%20Instance%20Segmentation2.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch2/02_02_two_stage.html">
   A. Detector based Method
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch2/02_01_01_YOLACT.html">
     YOLACT - ICCV 19
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch2/02_01_02_SOLO.html">
     SOLO - ECCV 20
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch2/02_02_01_MaskRCNN.html">
     [Draft] Mask R-CNN - ICCV 2017
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch2/02_02_03_Deep-MAC.html">
     [Draft] Deep-MAC - ICCV 2017
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch2/02_02_02_HTC.html">
     [Draft] HTC - CVPR 2019
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch2/02_02_04_BCNet.html">
     [Draft] BCNet - CVPR 2021
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Panoptic Segmentation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch3/03_Panoptic%20Segmentation2.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch3/03_01_Box.html">
   A. Box based method
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch3/03_01_01_PanopticFPN.html">
     PanopticFPN - CVPR 2019
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch3/03_02_02_DETR.html">
     DETR - ECCV 2020
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch3/03_01_02_PanopticFCN.html">
     [Draft] PanopticFCN - CVPR 2021
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch3/03_02_Box_Free.html">
   B. Box-free Method
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch3/03_02_01_Max-DeepLab.html">
     MaX-DeepLab - CVPR 21
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Weakly-supervised Segmentation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch4/04_Weakly-supervised%20Segmentation2.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch4/04_01_LearningPixelAffinity.html">
   A. Learning Pixel Affinity
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/04_01_01_AffinityNet.html">
     AffinityNet - CVPR 2018
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/04_01_02_IAL.html">
     IAL - IJCV 2020
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch4/04_02_AdversarialErasing.html">
   B. Adversarial Erasing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/04_02_01_AE-PSL.html">
     [Draft] AE-PSL - CVPR 2017
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/04_02_02_SeeNet.html">
     SeeNet - NeurIPS 2018
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Semi-supervised Segmentation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch5/05%20Semi-supervised%20Segmentation.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch5/05_01_Consistency_reg.html">
   A. Consistency Regularization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/05_01_01_CCT.html">
     CCT - CVPR 2020
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/05_01_02_SemiContexAware.html">
     Semi-supervised Semantic Segmentation with Directional Context-aware Consistency - CVPR 2021
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch5/05_02_GAN.html">
   B. GAN
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/05_02_01_Revisiting_CycleGAN.html">
     Revisiting CycleGAN for semi-supervised segmentation - 	arXiv:1908.11569
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/05_02_02_s4GAN.html">
     s4GAN - arXiv:1908.05724
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Appendix/DenseCRF.html">
   Tutorial: DenseCRF for segmentation task
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/docs/ch1/01_03_01_SETR.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/Pseudo-Lab/SegCrew-Book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/Pseudo-Lab/SegCrew-Book/issues/new?title=Issue%20on%20page%20%2Fdocs/ch1/01_03_01_SETR.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contributions">
   Contributions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#proposed-method">
   Proposed Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sequentialization-and-position-embedding">
     1. Sequentialization and Position Embedding
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transformer-encoder">
     2. Transformer Encoder
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decoder">
     3. Decoder
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experiment-result">
   Experiment Result
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation-details">
     1. Implementation Details
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#setting">
     2. Setting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#result">
     3. Result
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="setr-cvpr-2021">
<h1>SETR - CVPR 2021<a class="headerlink" href="#setr-cvpr-2021" title="Permalink to this headline">¶</a></h1>
<hr class="docutils" />
<div class="admonition-information admonition">
<p class="admonition-title">Information</p>
<ul class="simple">
<li><p><strong>Title:</strong> [SEmantic TRansformer] Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers, CVPR 2021</p></li>
<li><p><strong>Reference</strong></p>
<ul>
<li><p>Paper : <a class="reference external" href="https://arxiv.org/abs/2012.15840">https://arxiv.org/abs/2012.15840</a></p></li>
<li><p>Code : <a class="reference external" href="https://github.com/fudan-zvg/SETR">https://github.com/fudan-zvg/SETR</a></p></li>
</ul>
</li>
<li><p><strong>Review By:</strong> Jongsu Choi</p></li>
<li><p><strong>Last updated on Jun. 4, 2022</strong></p></li>
</ul>
</div>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>CNN 기반 Semantic Segmentation</p>
<ul>
<li><p>기존 방식은 대부분 FCN(Fully Convolution Network)의 Encoder-Decoder로 구성되어 있다.</p>
<ul>
<li><p>Encoder : Feature Representation Learning을 역할 하며, 주로 Stacked Conv. Layers로 구성되어 있다.</p></li>
<li><p>Decoder : Pixel Level Classification 역할을 수행한다.</p></li>
</ul>
</li>
<li><p>Encoder에서 stacked convolution layers를 지남에 따라 얻어지는 feature map은 resolution은 감소하고 depth가 깊어짐에 따라, 점점 Spatial Information은 줄어들고, abstract한 정보를 학습하게 된다.</p></li>
<li><p>또한, 구조적인 관점에서 receptive field는 network의 depth에 선형적인 관계를 가진다. 따라서 넓은 receptive field를 갖도록 network를 구성하기 위해서 depth를 늘려야 하는데, 일정 depth 이상으로 layer를 추가 하는 것은 의미가 없다. 즉 receptive field가 구조적으로 제한이 있어 FCN은 long range context를 학습하기 어려운 문제가 있다.</p></li>
<li><p>이런 한계를 극복하고자 Atrous/Dilate Convolution 등 방법으로 receptive field를 늘리는 방법들을 사용하였지만, encoder-decoder 구조를 탈피하지 못하고, down-sampling 방식으로 인한 단점은 여전하다.</p></li>
</ul>
</li>
</ul>
<div class="figure align-default" id="markdown-fig">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/setr1.png"><img alt="figure1" class="bg-primary mb-1" src="../../_images/setr1.png" style="width: 500px;" /></a>
<p class="caption"><span class="caption-number">Fig. 31 </span><span class="caption-text">FCN architecture (source: arXiv:1411.4038)</span><a class="headerlink" href="#markdown-fig" title="Permalink to this image">¶</a></p>
</div>
<ul class="simple">
<li><p>SETR(“SE”mantic + “TR”ansformer)</p>
<ul>
<li><p>Encoder에 pure Transformer만을 사용한 모델을 제안.</p></li>
<li><p>CNN 기반 모델과 다르게 pooling, stride와 같은 이미지/feature down-sampling 없이Transformer encoder를 이용하여 global context를 학습하는 새로운 접근 방법을 제안.</p></li>
<li><p>Transformer 모델은 이미 NLP분야에서 성능을 입증하였고, ViT(Vision Transformer)를 통해 Image Classification에서 좋은 성능을 보였다. 이는 이미지 특징 추출을 위해 stacked convolution 구조를 통한 공간 정보는 압축(손해)하며 global context를 학습한다는 종래 방식이 필수가 아님을 증명하였다.
(Translation Equivariance + Locality를 Inductive Bais로 하는 Conv. 방식이 Image 분야의 가장 효율적이라는 기존 방법에서 탈피)</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="contributions">
<h2>Contributions<a class="headerlink" href="#contributions" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Transformer Encoder를 적용한 새로운 semantic segmentation 구조를 제안
(Encoder-Decoder -&gt; Sequence to Sequence)</p></li>
<li><p>다양한 복잡도를 가진 Decoder를 적용하여 Self Attention의 특징 추출 효과에 대한 광범위한 해석 수행함.</p></li>
<li><p>SOTA 성능 달성 (ADE20K:  50.28% mIoU  Pascal Context:  55.83% mIoU)</p></li>
</ul>
</div>
<div class="section" id="proposed-method">
<h2>Proposed Method<a class="headerlink" href="#proposed-method" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>SETR(”SE”gmentation + “TR”ansformers)</p>
<ul>
<li><p>모델은 크게 Sequentialization → Transformer → Decoder로 구성되어있으며, Decoder는 3가지 방식을 제안한다.</p></li>
</ul>
</li>
</ul>
<div class="figure align-default" id="id1">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/architecture.png"><img alt="figure2" class="bg-primary mb-1" src="../../_images/architecture.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">Fig. 32 </span><span class="caption-text">Schematic illustration of SETR (source: arXiv:2012.15840)</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<div class="section" id="sequentialization-and-position-embedding">
<h3>1. Sequentialization and Position Embedding<a class="headerlink" href="#sequentialization-and-position-embedding" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Transformer 모델을 사용하기 위해 <span class="math notranslate nohighlight">\(H \times W \times 3\)</span> resolution의 이미지를 <span class="math notranslate nohighlight">\(C\)</span> hidden channel size를 가지는 <span class="math notranslate nohighlight">\(L\)</span>개의 sequential vector 형태로 re-representation 함.</p></li>
<li><p>본 논문에서는 ViT와 동일하게 입력 영상을 <span class="math notranslate nohighlight">\(16 \times 16\)</span> 개의 patch로 분할한다. 각각의 patch를  flatten 한 후 linear projection하여 <span class="math notranslate nohighlight">\(C\)</span>차원으로 축소하여 결과 적으로 1차원 patch embedding의 sequence로  변환한다. (L X C, L = H/16 X W/16)</p></li>
<li><p>순서 정보를 주기 위해 각 patch에 해당하는 position embedding을 추가한다.</p></li>
</ul>
</div>
<div class="section" id="transformer-encoder">
<h3>2. Transformer Encoder<a class="headerlink" href="#transformer-encoder" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>기존 Transformer와 다른점은 다음과 같다.
(1) MSA(Multi-Head Self Attention)전 layer norm을 적용한다.<br />
(2) MLP(Multi-Layer Perceptron) layer를 추가.</p></li>
<li><p>24개의 Transformer Block Layer로 모델을 구성</p></li>
<li><p>본 모델의 구조는 이미지 Patch형태로 Re-representation되어 Transformer Input으로 활용되어 모든 레이어는 “Global Receptive field”를 가진다.</p>
<ul>
<li><p>각 Patch에 대한 attention score를 계산하기 위해 다른 모든 patch가 활용됨.
—&gt; 기존 FCN 형태의 Limited Receptive Field 한계를 개선</p></li>
</ul>
</li>
</ul>
<div class="figure align-default" id="id2">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/attention.png"><img alt="figure3" class="bg-primary mb-1" src="../../_images/attention.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 33 </span><span class="caption-text">Attention map of picked points (red) (source: arXiv:2012.15840)</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id3">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/result1.png"><img alt="figure4" class="bg-primary mb-1" src="../../_images/result1.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">Fig. 34 </span><span class="caption-text">Visualization of output features (source: arXiv:2012.15840)</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<ul class="simple">
<li><p>Auxiliary Loss : Transformer layer 중 해당 Index에 해당 되는 output을 <span class="math notranslate nohighlight">\([H \times W \times \text{num. of class}]\)</span>로 변환 후 auxiliary loss를 계산 (channel : 2D convolution. Resolution : Bilinear Interpolation)</p></li>
</ul>
</div>
<div class="section" id="decoder">
<h3>3. Decoder<a class="headerlink" href="#decoder" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default" id="id4">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/SETR_MLA.png"><img alt="figure5" class="bg-primary mb-1" src="../../_images/SETR_MLA.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">Fig. 35 </span><span class="caption-text">SETR-MLA (source: arXiv:2012.15840)</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<ul>
<li><p>Pixel Level Segmentation을 위해 Transformer Encoder의 Output H/16 X W/16 X C Feature Map을 <span class="math notranslate nohighlight">\([H \times W \times \text{num. of class}]\)</span> 형태의 Segmentation Map 형태로 Re-representation 하는 역할</p></li>
<li><p>우선 transformer output <span class="math notranslate nohighlight">\(Z\)</span> (dim.: <span class="math notranslate nohighlight">\(\frac{HW}{256} \times C\)</span>)를 <span class="math notranslate nohighlight">\(\frac{H}{16} \times \frac{W}{16} \times C\)</span>로 reshape 후 다음과 같은 3가지 decoder를 적용하여 최종 segment map을 도출한다.</p>
<p>(1) Naive Decoder</p>
<ul class="simple">
<li><p>가장 기본적인 방법으로 우선 채널을 # of classes로 맞춰준 후 bi-linear interpolation을 통해 <span class="math notranslate nohighlight">\([H \times W \times \text{num. of class}]\)</span> segmentation map을 만들어 줌</p></li>
</ul>
<p>(2) Progressive UP-sampling(PUP) :</p>
<ul class="simple">
<li><p>One-step up-scaling은 많은 noise를 발생하므로 한번에 2배씩 총 4번 up-scale하는 방법을 적용한다.</p></li>
</ul>
<p>(3) Multi_Level feature Aggregation(MLA)</p>
<ul class="simple">
<li><p>FPN(Feature Pyramid Network)와 유사한 Multi-Level Feature를 사용하는데, 비슷한 개념을 차용하여, 24개의 Transformer Block Layer에서 추출하여 Multi-level Feature 개념으로 사용한다.</p></li>
<li><p>전체 e개의 Layers중 M개의 Layers를 e/M Step 등 간격으로 추출 하여 feature로 사용한다. e=24, M = 4인경우 6, 12, 18, 24번째 Layer를 MLA에 사용한다.</p></li>
<li><p>각 Level에서의 Feature는 2D shape(<span class="math notranslate nohighlight">\(\frac{HW}{256} \times C\)</span>)을 <span class="math notranslate nohighlight">\(\frac{H}{16} \times \frac{W}{16} \times C\)</span> 형태의 3D feature map형태로 reshape되고 3개의 conv. laye를 거치며 channel을 축소한다. 이후 X4 resolution Up-Sampling(bi-Linear interpolation) 후 element wise aggregation한다. 이후 <span class="math notranslate nohighlight">\(3 \times 3\)</span> convolution을 통해  #Classes 만큼 Channel을 맞춰 준 후(128*4 → Num_Classes) X4 resolution upsampling을 통해 원본의 해상도 <span class="math notranslate nohighlight">\(H \times W\)</span>로 만들어 주어 최종 segmentation map을 만들어 낸다.</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="experiment-result">
<h2>Experiment Result<a class="headerlink" href="#experiment-result" title="Permalink to this headline">¶</a></h2>
<div class="section" id="implementation-details">
<h3>1. Implementation Details<a class="headerlink" href="#implementation-details" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>mmsegmentation 기반으로 구현</p></li>
<li><p>Augmentation:</p>
<ul>
<li><p>Random cropping (768, 512 and 480 for Cityscapes, ADE20K and Pascal Context respectively)</p></li>
<li><p>Random horizental flipping</p></li>
</ul>
</li>
<li><p>Optimization</p>
<ul>
<li><p>Init. learning rate: 0.001 (on ADE20K, Pascal Context), 0.01 (on Cityscapes)</p></li>
<li><p>SGD with polynomial learning rate decay schedule</p></li>
<li><p>Momentum = 0.9, weight decay =0</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="setting">
<h3>2. Setting<a class="headerlink" href="#setting" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Evaluation Dataset: Cityscapes, ADK20K, PASCAL Context</p></li>
<li><p>SETR variants</p>
<ul>
<li><p>T-Large, T-Base : Transformer Block 수와 Head 수를 조절 하여 T-Large(24EA, 16head), T-Base(12EA, 12Head)로 디자인.</p></li>
<li><p>ViT, DeiT : ViT or DeiT에서 제공하는 weight를 transformer와 linear projection layer에 사용. (그 외 parameters는 random initialize.)</p></li>
<li><p>Single-scale(SS), multi-scale (MS, image scaling(x0.5, x0.75, x1.0. x1.25, x1.5, x1.75) 적용 후 sliging window를 적용하여 test set 추출 (e.g. Pascal Context의 경우 480x480 크기로 crop)</p></li>
<li><p>Hybrid setting의 경우 ResNet-50 기반 FCN의 output을 SETR에 입력하는 형태로 구성함.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="result">
<h3>3. Result<a class="headerlink" href="#result" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Cityscape dataset을 이용하여 학습한 다양한 SETR model의 성능을 비교</p>
<ul>
<li><p>Decoder의 경우 점진적으로 upsampling을 수행하는 SETR-PUP가 가장 좋은 성능을 나타냄.
→ SETR-MLA와 같이 서로 다른 transformer layers에서의 output이 FPN과 같이 장점을 가지지 않음.</p></li>
<li><p>T-Large setting에서 SETR-PUP-Base는 hybrid setting 대비 80K로 학습할 때 높은 성능을 나타냄. 이는 FCN encoder를 Transformer가 대체할 수 있음을 나타냄.</p></li>
</ul>
</li>
</ul>
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/setr2.png"><img alt="figure6.png" class="bg-primary mb-1 align-center" src="../../_images/setr2.png" style="width: 400px;" /></a>
<ul class="simple">
<li><ul>
<li><p>ADE20K val. set에서는 SETR-MLE가 약간의 성능 우위를 보임.</p></li>
<li><p>DeiT pre-train 모델을 사용한 경우 더 좋은 성능을 나타냄 → pre-train 선택이 중요함.</p></li>
</ul>
</li>
</ul>
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/setr3.png"><img alt="figure7.png" class="bg-primary mb-1 align-center" src="../../_images/setr3.png" style="width: 400px;" /></a>
<ul class="simple">
<li><p>SOTA 방법과 비교</p>
<ul>
<li><p>ADE20K dataset에서 기존 제안된 segmentation 대비 SETR-MLA 방법이 가장 높은 mIoU를 가짐을 확인 가능하며, multi-scale inference에서 SOTA를 달성했음을 알 수 있음.</p></li>
<li><p>Pascal Context dataset에서 FCN 모델 대비 월등히 높은 성능을 보임.</p></li>
</ul>
</li>
</ul>
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/result2.png"><img alt="figure8.png" class="bg-primary mb-1 align-center" src="../../_images/result2.png" style="width: 800px;" /></a>
<ul class="simple">
<li><ul>
<li><p>Cityscapes dataset에서 SETR이 FCN 방법대비 높은 성능을 나타내고, SOTA로 알려진 Non-local 및 CCNet과 같은 방법보다 높음 성능을 나타냄.</p></li>
</ul>
</li>
</ul>
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/result3.png"><img alt="figure9.png" class="bg-primary mb-1 align-center" src="../../_images/result3.png" style="width: 800px;" /></a>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/ch1"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="01_03_transformer.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">C. Transformer based method</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="../ch2/02%20Instance%20Segmentation2.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Overview</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By PseudoLab<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>